import os
import time
import lyricsgenius
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from local_agent import LocalAgent
import httpx
from langchain_core.prompts import PromptTemplate

# Token Genius (mantenha seguro em vari√°veis de ambiente em produ√ß√£o)
GENIUS_TOKEN = os.environ.get("GENIUS_TOKEN", "x5lnU50yJ34O0nmdOD9NszzrxXl8iz-d_9F8yr1JmSJ7z2MaglqCYwh7gztl1_pW")

genius = lyricsgenius.Genius(GENIUS_TOKEN, timeout=30)

template = """
Voc√™ √© um expert de letras de m√∫sica, sempre responda em portugu√™s do Brasil. Voc√™ sabe todas as letras de todas as m√∫sicas e devolve as letras das m√∫sicas com base no t√≠tulo.


Aqui est√° a letra da m√∫sica:
{reviews}

Aqui est√° o t√≠tulo da m√∫sica para responder:
{question}
"""

prompt = ChatPromptTemplate.from_template(template)
# Modelo Ollama configur√°vel via vari√°vel de ambiente OLLAMA_MODEL.
# Por padr√£o tentamos 'qwen3:4b', mas se o modelo n√£o estiver instalado
# voc√™ pode definir OLLAMA_MODEL para o nome listado por `ollama list`,
# por exemplo 'qwen3-coder:480b-cloud'.
MODEL_NAME = os.environ.get("OLLAMA_MODEL", "qwen3:4b")
model = OllamaLLM(model=MODEL_NAME)
chain = prompt | model

# Inicializa agente local de index/search (n√£o cr√≠tico ‚Äî falhar√° com mensagem)
try:
    agent = LocalAgent()
except Exception as e:
    print(f"Aviso: n√£o foi poss√≠vel inicializar LocalAgent: {e}")
    agent = None


def buscar_letra_genius(musica, artista=""):
    print(f"Buscando letra para '{musica}' - '{artista}' na Genius...")
    start = time.time()
    try:
        if artista:
            song = genius.search_song(musica, artista)
        else:
            song = genius.search_song(musica)
        if song and song.lyrics:
            tempo = time.time() - start
            print(f"Letra encontrada em {tempo:.2f} segundos." )
            return song.lyrics
        else:
            tempo = time.time() - start
            print(f"Letra n√£o encontrada ap√≥s {tempo:.2f} segundos.")
    except Exception as e:
        tempo = time.time() - start
        print(f"Falha ao buscar na Genius ap√≥s {tempo:.2f} segundos: {e}")
    return None


def extrair_artista_musica(entrada):
    entrada = entrada.lower()
    if " de " in entrada:
        musica, artista = entrada.split(" de ", 1)
        return artista.strip(), musica.strip()
    else:
        return "", entrada.strip()


def safe_invoke(payload: dict):
    """Tenta invocar o chain; em caso de falha de conex√£o ao cliente HTTP
    (por exemplo Ollama n√£o rodando), devolve um fallback amig√°vel com a
    letra (se presente) para que o bot continue funcionando.
    """
    try:
        return chain.invoke(payload)
    except httpx.ConnectError:
        # Falha ao conectar com o server Ollama/local LLM
        reviews = payload.get("reviews") or ""
        question = payload.get("question") or ""
        return f"[FALLBACK] N√£o foi poss√≠vel conectar ao modelo local (Ollama).\n\nPergunta: {question}\n\nConte√∫do dispon√≠vel:\n{reviews}"
    except Exception as e:
        msg = str(e)
        # detect common Ollama model-not-found error and give actionable advice
        if "not found" in msg and "model" in msg:
            return (
                f"[ERRO] Modelo n√£o encontrado: {MODEL_NAME}.\n"
                "Verifique os modelos instalados com `ollama list` e, se quiser baixar o modelo, rode:\n"
                f"  ollama pull {MODEL_NAME}\n\n"
                "Ou defina a vari√°vel de ambiente OLLAMA_MODEL para um modelo j√° instalado, por exemplo:\n"
                "  setx OLLAMA_MODEL \"qwen3-coder:480b-cloud\"\n"
                "e reinicie o terminal/sess√£o antes de rodar novamente."
            )
        return f"[ERRO] Falha ao gerar resposta: {msg}"

MODELO_CURIOSIDADE = "phi3:mini"

template_curiosidade = """
Voc√™ √© um especialista em m√∫sica.
Responda em portugu√™s do Brasil com UMA √∫nica curiosidade ou fato interessante sobre a m√∫sica "{musica}" do artista "{artista}".
Seja direto e breve (m√°ximo 2 frases). Se n√£o souber, apenas diga que n√£o encontrou fatos.

Curiosidade:
"""
prompt_curiosidade = PromptTemplate.from_template(template_curiosidade)

# 3. Crie o novo LLM e o novo Chain
# (Podemos definir um timeout menor aqui, pois a tarefa √© r√°pida)
try:
    llm_curiosidade = OllamaLLM(model=MODELO_CURIOSIDADE, request_timeout=30.0)
    chain_curiosidade = prompt_curiosidade | llm_curiosidade
except Exception as e:
    print(f"AVISO: N√£o foi poss√≠vel carregar o modelo de curiosidade ({MODELO_CURIOSIDADE}). {e}")
    chain_curiosidade = None

# 4. Crie uma nova fun√ß√£o "safe_invoke" para a curiosidade
def safe_invoke_curiosidade(musica, artista):
    if not chain_curiosidade:
        return "" # Retorna vazio se o modelo n√£o carregou

    try:
        payload = {"musica": musica, "artista": artista}
        resposta = chain_curiosidade.invoke(payload)
        
        # Limpa a resposta para evitar lixo
        resposta = str(resposta).strip()
        if not resposta or "n√£o encontrei" in resposta.lower() or "n√£o sei" in resposta.lower():
            return ""
            
        return f"\n\nüí° **Curiosidade:**\n{resposta}"
        
    except Exception as e:
        print(f"Erro ao gerar curiosidade: {e}")
        return "" # Falha silenciosamente